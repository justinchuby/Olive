{
    "input_model":{
        "type": "PyTorchModel",
        "config": {
            "model_script": "code/user_script.py",
            "script_dir": "code",
            "hf_config": {
                "model_name": "meta-llama/Llama-2-7b-hf",
                "model_class": "LlamaForCausalLM",
                "task": "text-generation",
                "components" : [
                    {
                        "name": "decoder",
                        "io_config": "get_decoder_io_config",
                        "component_func": "get_decoder_component",
                        "dummy_inputs_func": "get_decoder_dummy_inputs"
                    },
                    {
                        "name": "decoder_with_past",
                        "io_config": "get_decoder_with_past_io_config",
                        "component_func": "get_decoder_with_past_component",
                        "dummy_inputs_func": "get_decoder_with_past_dummy_inputs"
                    }
                ]
            }
        }
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "config": {
                "accelerators": ["gpu"]
            }
        }
    },
    "passes": {
        "sharding": {
            "type": "Sharding",
            "config": {
                "world_size": 8,
                "target_opset": 17,
                "save_as_external_data": true,
                "all_tensors_to_one_file": true
            }
        }
    },
    "engine": {
        "log_severity_level": 0,
        "search_strategy": false,
        "evaluate_input_model": false,
        "target": {
            "type": "LocalSystem",
            "config": {
                "accelerators": ["gpu"]
            }
        },
        "execution_providers": ["CUDAExecutionProvider"],
        "cache_dir": "cache",
        "output_dir": "models/llama2"
    }
}
